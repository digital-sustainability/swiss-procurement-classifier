{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Suite\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from db import connection, engine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc, f1_score\n",
    "from sklearn.utils import shuffle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import helpers_new as fdn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('My pandas version is {}. Please use version 0.23.1'.format(pd.__version__))\n",
    "print('My numpy version is {}. Please use version 1.13.1'.format(np.__version__))\n",
    "# import sklearn\n",
    "# print('The scikit-learn version is {}. Please use version 0.20.1'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Attributes\n",
    "def cleanData(df, filters):\n",
    "    if 'beschaffungsstelle_plz' in filters:\n",
    "        df[['beschaffungsstelle_plz']] = df[['beschaffungsstelle_plz']].applymap(fdn.tonumeric)\n",
    "    if 'gatt_wto' in filters:\n",
    "        df[['gatt_wto']] = df[['gatt_wto']].applymap(fdn.unifyYesNo)\n",
    "    if 'preis' in filters:\n",
    "        df[['preis']] = df[['preis']].applymap(fdn.createPriceCategory)\n",
    "    if 'anzahl_angebote' in filters:\n",
    "        df[['anzahl_angebote']] = df[['anzahl_angebote']].applymap(fdn.tonumeric)\n",
    "    if 'teilangebote' in filters:\n",
    "        df[['teilangebote']] = df[['teilangebote']].applymap(fdn.unifyYesNo)\n",
    "    if 'lose' in filters:\n",
    "        df[['lose']] = df[['lose']].applymap(fdn.unifyYesNo)\n",
    "    if 'varianten' in filters:\n",
    "        df[['varianten']] = df[['varianten']].applymap(fdn.unifyYesNo)\n",
    "    if 'auftragsart_art' in filters:\n",
    "        vectorizer = CountVectorizer(binary=True)\n",
    "        X = vectorizer.fit_transform(df['auftrags_art'].values)\n",
    "        text_columns = vectorizer.get_feature_names()\n",
    "        title_df = pd.DataFrame(X.todense(), columns=text_columns)\n",
    "        df = pd.concat([df, title_df], axis=1)\n",
    "        df = df.drop('projekt_titel', axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareForRun(df_pos, df_neg_all, filterAttributes):\n",
    "    # What attributes the model will be trained by\n",
    "    filters = ['Y', 'meldungsnummer', 'ausschreibung_cpv'] + filterAttributes\n",
    "    df_ready_all = []\n",
    "    for df_neg in df_neg_all:\n",
    "        # Merge positive and negative df into one, only use selected attributes\n",
    "        df_tmp = df_pos.append(df_neg, ignore_index=True)[filters].copy()\n",
    "        # Clean the data of all selected attributes\n",
    "        df_tmp = cleanData(df_tmp, filterAttributes)\n",
    "        df_ready_all.append(df_tmp)\n",
    "    return df_ready_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, df in enumerate(dataFrame): # enum to get index\n",
    "        run = shuffle(df) \n",
    "        unique_mn = run.meldungsnummer.unique()\n",
    "        xUniqueTest, xUniqueTrain = train_test_split(unique_mn, test_size=test_size)\n",
    "        xAndYTest = run[run['meldungsnummer'].isin(xUniqueTest)].copy()\n",
    "        xAndYTrain = run[run['meldungsnummer'].isin(xUniqueTrain)].copy()\n",
    "        xtest = xAndYTest.iloc[:, 1:]\n",
    "        ytest = xAndYTest.iloc[:, 0]\n",
    "        xtrain = xAndYTrain.iloc[:, 1:]\n",
    "        ytrain = xAndYTrain.iloc[:, 0]\n",
    "        print('Unique_mn {}'.format(len(unique_mn)))\n",
    "        print('xUniqueTest {}'.format(len(xUniqueTest)))\n",
    "        print('xUniqueTrain {}'.format(len(xUniqueTrain)))\n",
    "        print('ytest {}'.format(len(ytest)))\n",
    "        print(xtest)\n",
    "        print('xtrain {}'.format(len(xtrain)))\n",
    "        print('xAndYTest {}'.format(len(xAndYTest)))\n",
    "        print('------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = shuffle(dataFrame[1]) # X times an indendical df. Each will be shuffeled\n",
    "unique_mn = run.meldungsnummer.unique()\n",
    "xUniqueTest, xUniqueTrain = train_test_split(unique_mn, test_size=test_size)\n",
    "xAndYTest = run[run['meldungsnummer'].isin(xUniqueTest)].copy()\n",
    "xAndYTrain = run[run['meldungsnummer'].isin(xUniqueTrain)].copy()\n",
    "xtest = xAndYTest.iloc[:, :]\n",
    "ytest = xAndYTest.iloc[:, 0]\n",
    "xtrain = xAndYTrain.iloc[:, :]\n",
    "ytrain = xAndYTrain.iloc[:, 0]\n",
    "duplicates = 0\n",
    "\n",
    "for x in xtest.values:\n",
    "    for y in xtrain.values:\n",
    "       # print('{} == {}'.format(x[1], y[1]))\n",
    "        if x[1] == y[1]:\n",
    "            duplicates += 1\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runDecisionTree(dataFrame, trees, depth, test_size):\n",
    "    xTests = [];\n",
    "    yTests = [];\n",
    "    for idx, df in enumerate(dataFrame): # enum to get index\n",
    "        run = shuffle(df) # X times an indendical df. Each will be shuffeled\n",
    "        \n",
    "        unique_mn = run.meldungsnummer.unique()\n",
    "    \n",
    "        xUniqueTest, xUniqueTrain = train_test_split(unique_mn, test_size=test_size)\n",
    "    \n",
    "        xAndYTest = run[run['meldungsnummer'].isin(xUniqueTest)].copy()\n",
    "        xAndYTrain = run[run['meldungsnummer'].isin(xUniqueTrain)].copy()\n",
    "        \n",
    "        xtest = xAndYTest.iloc[:, 2:]\n",
    "        ytest = xAndYTest.iloc[:, 0]\n",
    "        \n",
    "        xtrain = xAndYTrain.iloc[:, 2:]\n",
    "        ytrain = xAndYTrain.iloc[:, 0]\n",
    "        # train the model on training sets\n",
    "        #clf = tree.DecisionTreeClassifier()\n",
    "        clf = RandomForestClassifier(n_estimators=trees, max_depth=depth, random_state=0)\n",
    "        clf = clf.fit(xtrain, ytrain)\n",
    "        print(clf.score(xtrain, ytrain))    # TODO: Explain\n",
    "        # predict on the test sets\n",
    "        prediction = clf.predict(xtest)\n",
    "        # pandas.series to data frame\n",
    "        df_ytest = ytest.to_frame()\n",
    "        # add run number to df\n",
    "        df_ytest['run'] = idx\n",
    "        xtest['run'] = idx\n",
    "        # add prediction to df\n",
    "        df_ytest['prediction']= prediction\n",
    "        # add result of run to df\n",
    "        df_ytest['correct'] = df_ytest['prediction']==df_ytest['Y']\n",
    "        # add run to run arrays\n",
    "        xTests.append(xtest)\n",
    "        yTests.append(df_ytest)\n",
    "    return xTests, yTests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracies(dfys):  \n",
    "    res = pd.DataFrame(columns=['accuracy', 'f1_score', 'fn rate'])\n",
    "    for dfy in dfys:\n",
    "        acc = round(accuracy_score(dfy.Y, dfy.prediction), 4)\n",
    "        f1 = round(f1_score(dfy.Y, dfy.prediction), 4)\n",
    "        cm = confusion_matrix(dfy.Y, dfy.prediction)\n",
    "        fnr = round(cm[1][0] / (cm[1][1] + cm[1][0]), 4)\n",
    "        res.loc[len(res)] = [ acc*100, f1*100, fnr*100 ] # add row to end of df, *100 for better % readability\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getConfusionMatices(dfys):  \n",
    "    res = pd.DataFrame(columns=['tn', 'tp', 'fp', 'fn'])\n",
    "    for dfy in dfys:\n",
    "        # ConfusionMatrix legende:\n",
    "        # [tn, fp]\n",
    "        # [fn, tp]\n",
    "        cm = confusion_matrix(dfy.Y, dfy.prediction)\n",
    "        res.loc[len(res)] = [ cm[0][0], cm[1][1], cm[0][1], cm[1][0] ]\n",
    "    res.loc['sum'] = res.sum() # Summarize each column\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Institution & Get Data\n",
    "Only needs to be done once per bidder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a bidder to train a model for (number of positive marked after the name)\n",
    "\n",
    "#anbieter = 'Arnold AG' #1006\n",
    "#anbieter = 'Alpiq AG' #827\n",
    "#anbieter = 'Siemens AG' #641\n",
    "#anbieter = 'Marti AG' #621\n",
    "#anbieter = 'Swisscom' #602\n",
    "#anbieter = 'Axpo AG' #577\n",
    "#anbieter = 'Hewlett-Packard' #155\n",
    "#anbieter = 'BG Ing√©nieurs Conseils' SA #151\n",
    "#anbieter = 'Pricewaterhousecoopers' # 92\n",
    "anbieter = 'Helbling Beratung + Bauplanung AG' #67\n",
    "#anbieter = 'Ofrex SA' #40\n",
    "#anbieter = 'PENTAG Informatik AG' #40\n",
    "#anbieter = 'Wicki Forst AG' #30\n",
    "#anbieter = 'T-Systems Schweiz' #30\n",
    "#anbieter = 'Bafilco AG' #20\n",
    "#anbieter = '4Video-Production GmbH' #20\n",
    "#anbieter = 'Widmer Ingenieure AG' #10\n",
    "#anbieter = 'hmb partners AG' #10\n",
    "#anbieter = 'Planmeca' #5\n",
    "#anbieter = 'K & M Installationen AG' #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_anbieter = (\n",
    "    \"anbieter.anbieter_id, \"\n",
    "    \"anbieter.institution as anbieter_institution, \"\n",
    "    \"cpv_dokument.cpv_nummer as anbieter_cpv, \"\n",
    "    \"ausschreibung.meldungsnummer\"\n",
    ")\n",
    "# anbieter_CPV are all the CPVs the Anbieter ever won a procurement for. So all the CPVs they are interested in. \n",
    "select_ausschreibung = (\n",
    "    \"anbieter.anbieter_id, \"\n",
    "    \"auftraggeber.institution as beschaffungsstelle_institution, \"\n",
    "    \"auftraggeber.beschaffungsstelle_plz, \"\n",
    "    \"ausschreibung.gatt_wto, \"\n",
    "    \"ausschreibung.sprache, \"\n",
    "    \"ausschreibung.auftragsart_art, \"\n",
    "    \"ausschreibung.lose, \"\n",
    "    \"ausschreibung.teilangebote, \"\n",
    "    \"ausschreibung.varianten, \"\n",
    "  #  \"ausschreibung.titel, \" TODO: Projektbeschrieb\n",
    "    \"ausschreibung.bietergemeinschaft, \"\n",
    "    \"cpv_dokument.cpv_nummer as ausschreibung_cpv, \"\n",
    "    \"ausschreibung.meldungsnummer as meldungsnummer2\"\n",
    ")\n",
    "\n",
    "def getResponses(select_anbieter, select_ausschreibung, bidder, response):\n",
    "    resp = '=';\n",
    "    if (not response):\n",
    "        resp = '!='\n",
    "    query = \"\"\"SELECT * FROM (SELECT {} from ((((((beruecksichtigteanbieter_zuschlag\n",
    "            INNER JOIN zuschlag ON zuschlag.meldungsnummer = beruecksichtigteanbieter_zuschlag.meldungsnummer)\n",
    "            INNER JOIN anbieter ON beruecksichtigteanbieter_zuschlag.anbieter_id = anbieter.anbieter_id)\n",
    "            INNER JOIN projekt ON zuschlag.projekt_id = projekt.projekt_id)\n",
    "            INNER JOIN auftraggeber ON projekt.auftraggeber_id = auftraggeber.auftraggeber_id)\n",
    "            INNER JOIN ausschreibung ON projekt.projekt_id = ausschreibung.projekt_id)\n",
    "            INNER JOIN cpv_dokument ON cpv_dokument.meldungsnummer = ausschreibung.meldungsnummer)\n",
    "            WHERE anbieter.institution {} \"{}\" ) anbieter\n",
    "        JOIN (SELECT {} from ((((((beruecksichtigteanbieter_zuschlag\n",
    "            INNER JOIN zuschlag ON zuschlag.meldungsnummer = beruecksichtigteanbieter_zuschlag.meldungsnummer)\n",
    "            INNER JOIN anbieter ON beruecksichtigteanbieter_zuschlag.anbieter_id = anbieter.anbieter_id)\n",
    "            INNER JOIN projekt ON zuschlag.projekt_id = projekt.projekt_id)\n",
    "            INNER JOIN auftraggeber ON projekt.auftraggeber_id = auftraggeber.auftraggeber_id)\n",
    "            INNER JOIN ausschreibung ON projekt.projekt_id = ausschreibung.projekt_id)\n",
    "            INNER JOIN cpv_dokument ON cpv_dokument.meldungsnummer = ausschreibung.meldungsnummer)\n",
    "            WHERE anbieter.institution {} \"{}\"\n",
    "            ) ausschreibung ON ausschreibung.meldungsnummer2 = anbieter.meldungsnummer;\n",
    "    \"\"\".format(select_anbieter, resp, bidder, select_ausschreibung, resp, bidder)\n",
    "    # pd.read_sql(query, connection);\n",
    "    return query.replace('\\n', '')\n",
    "\n",
    "\n",
    "getResponses(select_anbieter, select_ausschreibung, anbieter, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_anbieter = (\n",
    "    \"anbieter.anbieter_id, \"\n",
    "    \"anbieter.institution as anbieter_institution, \"\n",
    "    \"cpv_dokument.cpv_nummer as anbieter_cpv, \"\n",
    "    \"ausschreibung.meldungsnummer\"\n",
    ")\n",
    "# anbieter_CPV are all the CPVs the Anbieter ever won a procurement for. So all the CPVs they are interested in. \n",
    "select_ausschreibung = (\n",
    "    \"anbieter.anbieter_id, \"\n",
    "    \"auftraggeber.institution as beschaffungsstelle_institution, \"\n",
    "    \"auftraggeber.beschaffungsstelle_plz, \"\n",
    "    \"ausschreibung.gatt_wto, \"\n",
    "    \"ausschreibung.sprache, \"\n",
    "    \"ausschreibung.auftragsart_art, \"\n",
    "    \"ausschreibung.lose, \"\n",
    "    \"ausschreibung.teilangebote, \"\n",
    "    \"ausschreibung.varianten, \"\n",
    "  #  \"ausschreibung.titel, \" TODO: Projektbeschrieb\n",
    "    \"ausschreibung.bietergemeinschaft, \"\n",
    "    \"cpv_dokument.cpv_nummer as ausschreibung_cpv, \"\n",
    "    \"ausschreibung.meldungsnummer as meldungsnummer2\"\n",
    ")\n",
    "# Get all positive and negative responses\n",
    "responses_positive, full_negative = fdn.createAnbieterDf(select_anbieter, select_ausschreibung, anbieter)\n",
    "responses_positive.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***ToDo:***\n",
    "* TEST WITH BIDDERS OF DIFFERENT SIZE!\n",
    "* Add Auftrags_art as catagory!!\n",
    "* Create function to read all which tenderings are FPs / FNs\n",
    "* Create a n times n input for all attributes\n",
    "* Test with Random Forest\n",
    "* Enable better auto Evaluation Feedback. Maybe some Graphs or something\n",
    "* Prepare (and test with) more Attributes\n",
    "* Test Model only with Tenderings from the same ream / CPV category\n",
    "* (\"ctrl + F\" all TODOs in this file)\n",
    "* (Take a look at warning when tree is run)\n",
    "\n",
    "***Fragen:***\n",
    "* M√ºssen wir die Freih√§nder beachten?\n",
    "* Spielen Attribute des Zuschlags √ºberhaupt eine Rolle?\n",
    "* Welche Attribute wollen wir noch anschauen / einbringen?\n",
    "* Wenn wir nur den CPV verwenden, ist die Anwendung besser als ein normaler Filter?\n",
    "* K√∂nnte unser Algorithmus einen Bias haben, da wir mehrer CPV miteinander kombinieren, wenn wir die Tables laden?\n",
    "\n",
    "***Notes Zwischenpr√§sentation***\n",
    "* Design Science Vorgehen erweitern\n",
    "* Wie kann man Vertrauen in einen Recommendation Algorithmus schaffen? --> M√∂glichst tansparent dem User sagen, was wird in Betracht gezogen\n",
    "* Kann man die Wichtigkeit von einzelnen Attributen werten? / Gewichten\n",
    "* Der Algorithmus muss \"resetbar\" sein\n",
    "* Nick L√ºthi: Transparenz und Einsicht in Algorithmus? Wie kommt er auf das Ergebnis? Der Markt muss genung Vertauen in den Vorgang haben.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†ratio that the positive and negative responses have to each other\n",
    "positive_to_negative_ratio = 2/3\n",
    "# Percentage of training set that is used for testing (Recommendation of at least 25%)\n",
    "test_size = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train n different models on n different (reproducable) sample sizes\n",
    "runs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attributes ready for use: 'beschaffungsstelle_plz', 'gatt_wto', 'lose', 'teilangebote', 'varianten'\n",
    "\n",
    "# Next focus: 'beschaffungsstelle_institution', 'titel', 'sprache', 'auftragsart_art' <-- AUFTRAGSART!\n",
    "\n",
    "# ???: 'Preis', 'anzahl_angebote'\n",
    "\n",
    "#attributes = [ 'gatt_wto', 'lose', 'teilangebote', 'varianten']\n",
    "attributes = [ 'gatt_wto', 'lose', 'teilangebote', 'varianten', 'beschaffungsstelle_plz']\n",
    "attributes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune Random Forest Parameter\n",
    "trees = 100\n",
    "depth = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†create the chosen amount reproducable samples for negative DataFrames with the ratio definded above\n",
    "responses_negative_all = fdn.createNegativeResponses(\n",
    "    full_negative,\n",
    "    len(responses_positive),\n",
    "    runs,\n",
    "    positive_to_negative_ratio)\n",
    "\n",
    "# Assign positive and negative lables to both DFs\n",
    "responses_positive['Y'] = 1\n",
    "for df in responses_negative_all:\n",
    "    df['Y'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = prepareForRun(responses_positive, responses_negative_all, attributes)\n",
    "responses_positive.head()\n",
    "print(attributes)\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_test_train_split(dataFrame, test_size):\n",
    "\n",
    "    for idx, df in enumerate(dataFrame): # enum to get index\n",
    "        df\n",
    "        run = shuffle(df)\n",
    "        # Put responses in one arry and all desired properties in another\n",
    "        #y = run.iloc[:, 0]\n",
    "        #x = run.iloc[:, 1:] # Every column but the first\n",
    "    \n",
    "        unique_mn = run.meldungsnummer.unique()\n",
    "    \n",
    "        xUniqueTest, xUniqueTrain = train_test_split(unique_mn, test_size=test_size)\n",
    "    \n",
    "        xAndYTest = run[run['meldungsnummer'].isin(xUniqueTest)].copy()\n",
    "        xAndYTrain = run[run['meldungsnummer'].isin(xUniqueTrain)].copy()\n",
    "        \n",
    "        xTest = xAndYTest.iloc[:, 1:]\n",
    "        yTest = xAndYTest.iloc[:, 0]\n",
    "        \n",
    "        xTrain = xAndYTrain.iloc[:, 1:]\n",
    "        yTrain = xAndYTrain.iloc[:, 0]\n",
    "        \n",
    "        return yTest\n",
    "\n",
    "    #total_number = len(unique_mn)\n",
    "    \n",
    "    #number_of_test_items = int(total_number * test_size)\n",
    "    \n",
    "    #test_items = []\n",
    "    \n",
    "    #for i in range(number_of_test_items):\n",
    "    #    random.shuffle(unique_mn)\n",
    "    \n",
    "# unique_test_train_split(dataFrame, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # hide some \"slice of copy\" warnings\n",
    "xTests, yTests = runDecisionTree(prepareForRun(responses_positive, responses_negative_all, attributes), trees, depth, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attributes)\n",
    "pd.concat([getConfusionMatices(yTests), getAccuracies(yTests)], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get(tp/tn, run) --> no run: get all runs\n",
    "#¬†get all IDs of FPs and FNs in list\n",
    "# get corresponding attributes from outer list: CPV, CPV-description, title of tendering\n",
    "# for each, add to DF\n",
    "xTests[0].loc[1066,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
